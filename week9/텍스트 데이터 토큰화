텍스트 데이터 토큰화

#코드
#Spyder 사용했습니다

from nltk import *

#패키지 호출
from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer
#입력테스트 정의
input_text = "Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out."
#입력 텍스트를 문장 토큰으로 나눔
print("\nSentence Tokenizer:")
print(sent_tokenize(input_text))
#입력 텍스트를 단어 토큰으로 나눔
print("\nWord tokenizer : ")
print(word_tokenize(input_text)) 
#입력 텍스트를 word punct 사용해서 단어 토큰으로 나눔
print("\nWord Punct Tokenizer : ")
print(WordPunctTokenizer().tokenize(input_text)) 


#결과

runfile('C:/Users/Administrator/.spyder-py3/temp.py', wdir='C:/Users/Administrator/.spyder-py3')

Sentence Tokenizer:
['Do you know how tokenization works?', "It's actually quite interesting!", "Let's analyze a couple of sentences and figure it out."]

Word tokenizer : 
['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', "'s", 'actually', 'quite', 'interesting', '!', 'Let', "'s", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']

Word Punct Tokenizer : 
['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', "'", 's', 'actually', 'quite', 'interesting', '!', 'Let', "'", 's', 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']
